['winequality-red.csv']
confusion_matrix :
 [[285   5]
 [ 21   9]] 


classification_report:

              precision    recall  f1-score   support

           0       0.93      0.98      0.96       290
           1       0.64      0.30      0.41        30

   micro avg       0.92      0.92      0.92       320
   macro avg       0.79      0.64      0.68       320
weighted avg       0.90      0.92      0.91       320



#91% accuracy
